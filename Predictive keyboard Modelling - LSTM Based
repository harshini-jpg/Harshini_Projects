
# FAST & OPTIMIZED PREDICTIVE KEYBOARD (LSTM)

import re
import numpy as np
import tensorflow as tf
import gradio as gr
import nltk

from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping
nltk.download('gutenberg')
nltk.download('punkt')

raw_text = gutenberg.raw('austen-sense.txt')
sentences = sent_tokenize(raw_text)

# Limit data for FAST training
sentences = sentences[:2000]

# Text Preprocessing

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    return text

corpus = [clean_text(s) for s in sentences if len(s.split()) > 3]


#  Tokenization 

VOCAB_SIZE = 5000

tokenizer = Tokenizer(num_words=VOCAB_SIZE)
tokenizer.fit_on_texts(corpus)


# Sequence Creation

sequences = []

for sentence in corpus:
    token_list = tokenizer.texts_to_sequences([sentence])[0]
    for i in range(1, len(token_list)):
        sequences.append(token_list[:i+1])


# Padding

max_seq_len = max(len(seq) for seq in sequences)

sequences = pad_sequences(
    sequences, maxlen=max_seq_len, padding='pre'
)

X = sequences[:, :-1]
y = sequences[:, -1]
y = tf.keras.utils.to_categorical(y, num_classes=VOCAB_SIZE)


# OPTIMIZED LSTM Model

model = Sequential([
    Embedding(VOCAB_SIZE, 32, input_length=max_seq_len-1),
    LSTM(50),
    Dense(VOCAB_SIZE, activation='softmax')
])

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)


#   TRAINING
early_stop = EarlyStopping(
    monitor='loss',
    patience=3,
    restore_best_weights=True
)

model.fit(
    X, y,
    epochs=30,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# Save model (important)
model.save("predictive_keyboard_fast.h5")
print(" Model trained & saved")


#  Prediction Function

def predict_next_words(text, top_k=3):
    text = clean_text(text)
    token_list = tokenizer.texts_to_sequences([text])[0]

    if len(token_list) == 0:
        return "Enter meaningful text"

    token_list = pad_sequences(
        [token_list], maxlen=max_seq_len-1, padding='pre'
    )

    predictions = model.predict(token_list, verbose=0)[0]
    top_indices = predictions.argsort()[-top_k:][::-1]

    predicted_words = []
    for idx in top_indices:
        for word, index in tokenizer.word_index.items():
            if index == idx:
                predicted_words.append(word)
                break

    return ", ".join(predicted_words)


#  Gradio UI

interface = gr.Interface(
    fn=predict_next_words,
    inputs=gr.Textbox(
        label="Type text",
        placeholder="e.g. i am going to"
    ),
    outputs=gr.Textbox(label="Next word predictions"),
    title="Fast Predictive Keyboard (LSTM)",
    description="Optimized next-word prediction using real text and LSTM"
)


# Launch App

interface.launch()
