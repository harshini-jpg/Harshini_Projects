# -*- coding: utf-8 -*-
"""Resume_To_Video.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xT229qYuxbNoO-WEyoS8P5kz-xR5o8VZ
"""

# ============================================================
# MODULE 1: Resume Ingestion, Cleaning & Segmentation (NLP)
# Single Google Colab Cell
# ============================================================

# ---------- INSTALL DEPENDENCIES ----------
!pip install pdfplumber

# ---------- IMPORTS ----------
import pdfplumber
import re
from google.colab import files

# ---------- STEP 1: UPLOAD RESUME PDF ----------
uploaded = files.upload()
resume_path = list(uploaded.keys())[0]
print(f"‚úÖ Uploaded Resume: {resume_path}")

# ---------- STEP 2: EXTRACT TEXT FROM PDF ----------
def extract_resume_text(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

raw_text = extract_resume_text(resume_path)

print("\n========== RAW RESUME TEXT (First 500 chars) ==========\n")
print(raw_text[:500])

# ---------- STEP 3: CLEAN & NORMALIZE TEXT ----------
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[‚Ä¢‚óè‚ñ™]', '', text)     # remove bullet symbols
    text = re.sub(r'\t+', ' ', text)
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n+', '\n', text)
    text = re.sub(r'[^a-z0-9.,\n ]', '', text)
    return text.strip()

cleaned_text = clean_text(raw_text)

print("\n========== CLEANED TEXT (First 500 chars) ==========\n")
print(cleaned_text[:500])

# ---------- STEP 4: SEGMENT RESUME INTO SECTIONS ----------
def segment_resume(text):
    sections = {
        "education": [],
        "skills": [],
        "projects": [],
        "experience": []
    }

    current_section = None

    for line in text.split("\n"):
        line = line.strip()

        if any(k in line for k in ["education", "academic"]):
            current_section = "education"
            continue

        elif any(k in line for k in ["skills", "technical skills", "tools"]):
            current_section = "skills"
            continue

        elif any(k in line for k in ["project", "projects"]):
            current_section = "projects"
            continue

        elif any(k in line for k in ["experience", "internship", "work"]):
            current_section = "experience"
            continue

        if current_section and line:
            sections[current_section].append(line)

    # Convert lists to clean strings
    for key in sections:
        sections[key] = " ".join(sections[key])

    return sections

resume_sections = segment_resume(cleaned_text)

# ---------- STEP 5: DISPLAY STRUCTURED OUTPUT ----------
print("\n========== STRUCTURED RESUME OUTPUT ==========\n")

for section, content in resume_sections.items():
    print(f"\n--- {section.upper()} ---")
    print(content[:800])  # limit for readability

# ---------- FINAL CHECKPOINT ----------
print("\n‚úÖ MODULE 1 COMPLETED")
print("Output is now NLP-ready for skill extraction & summarization")

# ============================================================
# NLP PROJECT: Resume Processing & Skill Extraction
# MODULE 1 + MODULE 2 (Single Google Colab Cell)
# ============================================================

# ---------- INSTALL DEPENDENCIES ----------
!pip install pdfplumber

# ---------- IMPORTS ----------
import pdfplumber
import re
from google.colab import files

# ============================================================
# MODULE 1: RESUME INGESTION & PREPROCESSING
# ============================================================

# ---------- STEP 1: UPLOAD RESUME PDF ----------
uploaded = files.upload()
resume_path = list(uploaded.keys())[0]
print(f"\n‚úÖ Uploaded Resume: {resume_path}")

# ---------- STEP 2: EXTRACT TEXT FROM PDF ----------
def extract_resume_text(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

raw_text = extract_resume_text(resume_path)

print("\n========== RAW RESUME TEXT (Preview) ==========\n")
print(raw_text[:500])

# ---------- STEP 3: CLEAN & NORMALIZE TEXT ----------
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[‚Ä¢‚óè‚ñ™]', '', text)
    text = re.sub(r'\t+', ' ', text)
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n+', '\n', text)
    text = re.sub(r'[^a-z0-9.,\n ]', '', text)
    return text.strip()

cleaned_text = clean_text(raw_text)

print("\n========== CLEANED TEXT (Preview) ==========\n")
print(cleaned_text[:500])

# ---------- STEP 4: SEGMENT RESUME INTO SECTIONS ----------
def segment_resume(text):
    sections = {
        "education": [],
        "skills": [],
        "projects": [],
        "experience": []
    }

    current_section = None

    for line in text.split("\n"):
        line = line.strip()

        if any(k in line for k in ["education", "academic"]):
            current_section = "education"
            continue

        elif any(k in line for k in ["skills", "technical skills", "tools"]):
            current_section = "skills"
            continue

        elif any(k in line for k in ["project", "projects"]):
            current_section = "projects"
            continue

        elif any(k in line for k in ["experience", "internship", "work"]):
            current_section = "experience"
            continue

        if current_section and line:
            sections[current_section].append(line)

    for key in sections:
        sections[key] = " ".join(sections[key])

    return sections

resume_sections = segment_resume(cleaned_text)

print("\n========== STRUCTURED RESUME OUTPUT ==========\n")
for section, content in resume_sections.items():
    print(f"\n--- {section.upper()} ---")
    print(content[:700])

# ============================================================
# MODULE 2: SKILL & TOOL EXTRACTION (CORE NLP)
# ============================================================

# ---------- STEP 5: DEFINE SKILL VOCABULARY ----------
SKILL_VOCAB = [
    "python", "c", "c++", "java",
    "machine learning", "deep learning", "nlp",
    "transformers", "bert", "gpt",
    "pytorch", "tensorflow", "keras",
    "signal processing", "speech processing",
    "whisper", "opencv",
    "numpy", "pandas", "scikit-learn",
    "mongodb", "sql",
    "linux", "git", "github"
]

# ---------- STEP 6: COMBINE NLP-RELEVANT SECTIONS ----------
text_corpus = (
    resume_sections.get("skills", "") + " " +
    resume_sections.get("projects", "") + " " +
    resume_sections.get("experience", "")
).lower()

# ---------- STEP 7: SKILL MATCHING ----------
skill_counts = {}

for skill in SKILL_VOCAB:
    pattern = r"\b" + re.escape(skill) + r"\b"
    matches = re.findall(pattern, text_corpus)
    if matches:
        skill_counts[skill] = len(matches)

# ---------- STEP 8: CONTEXTUAL WEIGHTING ----------
weighted_skills = {}

for skill, count in skill_counts.items():
    weight = count

    if skill in resume_sections.get("projects", ""):
        weight += 2
    if skill in resume_sections.get("experience", ""):
        weight += 3

    weighted_skills[skill] = weight

# ---------- STEP 9: NORMALIZATION & RANKING ----------
max_weight = max(weighted_skills.values())

ranked_skills = [
    (skill, round(weight / max_weight, 2))
    for skill, weight in weighted_skills.items()
]

ranked_skills.sort(key=lambda x: x[1], reverse=True)

# ---------- STEP 10: DISPLAY FINAL NLP OUTPUT ----------
print("\n========== FINAL RANKED TECHNICAL SKILLS ==========\n")
for skill, score in ranked_skills:
    print(f"{skill:<22} ‚Üí importance score: {score}")

print("\n‚úÖ MODULE 1 + MODULE 2 COMPLETED SUCCESSFULLY")

import pdfplumber
import re
from google.colab import files
import spacy
from collections import Counter, defaultdict
import numpy as np
from sentence_transformers import SentenceTransformer, util

# -------------------- LOAD NLP MODELS --------------------
nlp = spacy.load("en_core_web_sm")
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# ============================================================
# MODULE 1: RESUME INGESTION & STRUCTURING
# ============================================================

# ---------- UPLOAD RESUME ----------
uploaded = files.upload()
resume_path = list(uploaded.keys())[0]
print(f"‚úÖ Uploaded: {resume_path}")

# ---------- TEXT EXTRACTION ----------
def extract_resume_text(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            if page.extract_text():
                text += page.extract_text() + "\n"
    return text

raw_text = extract_resume_text(resume_path)

# ---------- CLEANING ----------
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[‚Ä¢‚óè‚ñ™]', '', text)
    text = re.sub(r'\t+', ' ', text) # Replace tabs with spaces
    text = re.sub(r' +', ' ', text) # Replace multiple spaces with a single space
    text = re.sub(r'[^a-z0-9.,\n ]', '', text) # Keep newlines
    text = re.sub(r'\n+', '\n', text) # Replace multiple newlines with a single newline
    return text.strip()

cleaned_text = clean_text(raw_text)

# ---------- SEGMENTATION ----------
def segment_resume(text):
    sections = {"education": [], "skills": [], "projects": [], "experience": []}
    current = None

    for line in text.split("\n"):
        line = line.strip()

        if any(k in line for k in ["education", "academic"]):
            current = "education"
            continue

        elif any(k in line for k in ["skills", "technical skills", "tools"]):
            current = "skills"
            continue

        elif any(k in line for k in ["project", "projects"]):
            current = "projects"
            continue

        elif any(k in line for k in ["experience", "internship", "work"]):
            current = "experience"
            continue

        if current and line:
            sections[current].append(line)

    return {k: " ".join(v) for k, v in sections.items()}

resume_sections = segment_resume(cleaned_text)

# ============================================================
# MODULE 2 (UPGRADED): SEMANTIC SKILL EXTRACTION
# ============================================================

# ---------- STEP 1: DYNAMIC SKILL DISCOVERY (POS / NOUN PHRASES) ----------
def extract_candidate_skills(text):
    doc = nlp(text)
    phrases = []

    for chunk in doc.noun_chunks:
        phrase = chunk.text.strip()
        if 1 < len(phrase.split()) <= 4:
            phrases.append(phrase)

    return phrases

candidate_phrases = extract_candidate_skills(
    resume_sections["skills"] + " " +
    resume_sections["projects"] + " " +
    resume_sections["experience"]
)

# ---------- STEP 2: CANONICAL SKILL ANCHORS ----------
# (Used ONLY for semantic validation, not hard matching)

CANONICAL_SKILLS = [
    "python", "machine learning", "deep learning", "nlp",
    "transformers", "bert", "pytorch", "tensorflow",
    "speech processing", "signal processing",
    "whisper", "opencv", "scikit-learn",
    "mongodb", "sql", "git", "linux"
]

canonical_embeddings = embedder.encode(CANONICAL_SKILLS, convert_to_tensor=True)

# ---------- STEP 3: SEMANTIC SKILL MATCHING ----------
semantic_skills = defaultdict(list)

for phrase in candidate_phrases:
    phrase_emb = embedder.encode(phrase, convert_to_tensor=True)
    similarity = util.cos_sim(phrase_emb, canonical_embeddings)[0]
    best_idx = int(np.argmax(similarity))
    best_score = float(similarity[best_idx])

    if best_score > 0.6:
        canonical = CANONICAL_SKILLS[best_idx]
        semantic_skills[canonical].append((phrase, best_score))

# ---------- STEP 4: SECTION-AWARE WEIGHTING ----------
def section_weight(skill):
    weight = 1
    # Check if the skill is directly mentioned in the section content (as a sub-string)
    # This is a simplification; more advanced checks might involve NER or semantic checks
    if skill in resume_sections.get("projects", ""):
        weight += 2
    if skill in resume_sections.get("experience", ""):
        weight += 3
    return weight


# ---------- STEP 5: FINAL SCORING ----------
final_scores = {}

for skill, matches in semantic_skills.items():
    freq_score = len(matches)
    contextual_weight = section_weight(skill)
    confidence = np.mean([s for _, s in matches])
    final_scores[skill] = freq_score * contextual_weight * confidence

# ---------- NORMALIZATION ----------
# Handle the case where final_scores might still be empty if no skills are found
if final_scores:
    max_score = max(final_scores.values())

    ranked_skills = [
        (skill, round(score / max_score, 2))
        for skill, score in final_scores.items()
    ]

    ranked_skills.sort(key=lambda x: x[1], reverse=True)
else:
    ranked_skills = [] # No skills found, return an empty list or handle as appropriate

# ============================================================
# FINAL OUTPUT
# ============================================================

print("\n========== STRUCTURED RESUME ==========\n")
for k, v in resume_sections.items():
    print(f"\n--- {k.upper()} ---\n{v[:600]}")

print("\n========== SEMANTICALLY RANKED SKILLS ==========\n")
if ranked_skills:
    for skill, score in ranked_skills:
        print(f"{skill:<25} ‚Üí semantic importance: {score}")
else:
    print("No semantic skills found or ranked.")

print("\n‚úÖ NLP PIPELINE COMPLETED SUCCESSFULLY")

# ============================================================
# NLP PROJECT: Resume ‚Üí Semantic Skills ‚Üí Video Resume Script
# MODULE 1 + MODULE 2 + MODULE 3 (Single Colab Cell)
# ============================================================

# ---------------- INSTALLS ----------------
#!pip install pdfplumber spacy sentence-transformers
#!python -m spacy download en_core_web_sm

# ---------------- IMPORTS ----------------
import pdfplumber, re
from google.colab import files
import spacy
import numpy as np
from sentence_transformers import SentenceTransformer, util

# ---------------- MODELS ----------------
nlp = spacy.load("en_core_web_sm")
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# ============================================================
# MODULE 1: RESUME INGESTION
# ============================================================

uploaded = files.upload()
resume_path = list(uploaded.keys())[0]

def extract_text(pdf):
    text = ""
    with pdfplumber.open(pdf) as p:
        for page in p.pages:
            if page.extract_text():
                text += page.extract_text() + "\n"
    return text.lower()

raw_text = extract_text(resume_path)

def clean(text):
    text = re.sub(r'[‚Ä¢‚óè‚ñ™]', '', text)
    text = re.sub(r'[^a-z0-9.,\n ]', '', text)
    return re.sub(r'\s+', ' ', text)

cleaned = clean(raw_text)

def segment(text):
    sections = {"education": "", "skills": "", "projects": "", "experience": ""}
    current = None
    for line in text.split("\n"):
        if "education" in line: current = "education"
        elif "skill" in line: current = "skills"
        elif "project" in line: current = "projects"
        elif any(k in line for k in ["experience", "internship"]): current = "experience"
        elif current: sections[current] += line + " "
    return sections

resume = segment(cleaned)

# ============================================================
# MODULE 2: SEMANTIC SKILL INTELLIGENCE
# ============================================================

CANONICAL_SKILLS = [
    "python", "machine learning", "nlp", "transformers",
    "pytorch", "tensorflow", "speech processing",
    "signal processing", "whisper", "git"
]

skill_embeddings = embedder.encode(CANONICAL_SKILLS, convert_to_tensor=True)

doc = nlp(resume["skills"] + resume["projects"] + resume["experience"])

detected_skills = {}

for chunk in doc.noun_chunks:
    emb = embedder.encode(chunk.text, convert_to_tensor=True)
    sims = util.cos_sim(emb, skill_embeddings)[0]
    idx = int(np.argmax(sims))
    if sims[idx] > 0.6:
        skill = CANONICAL_SKILLS[idx]
        detected_skills[skill] = detected_skills.get(skill, 0) + 1

ranked_skills = sorted(detected_skills.items(), key=lambda x: x[1], reverse=True)

# ============================================================
# MODULE 3: SPOKEN VIDEO RESUME SCRIPT
# ============================================================

def generate_script(resume, skills):
    intro = (
        "Hello, I am Harshini, an electronics and communication engineering student "
        "with a strong focus on NLP and machine learning.\n\n"
    )

    skill_text = "My core technical strengths include "
    skill_text += ", ".join([s for s, _ in skills[:5]]) + ".\n\n"

    project_text = (
        "I have applied these skills in projects involving "
        "speech processing and intelligent language systems, "
        "where I focused on building practical and scalable solutions.\n\n"
    )

    closing = (
        "I am particularly interested in roles that allow me to work at the "
        "intersection of signal processing and NLP. Thank you for your time."
    )

    return intro + skill_text + project_text + closing

video_script = generate_script(resume, ranked_skills)

# ---------------- OUTPUT ----------------
print("\n===== SEMANTIC SKILLS =====")
for s in ranked_skills:
    print(s)

print("\n===== VIDEO RESUME SCRIPT =====\n")
print(video_script)

# ============================================================
# REAL-TIME TEXT-TO-SPEECH OUTPUT (NO AUDIO FILE)
# ============================================================

!pip install gTTS

from gtts import gTTS
from IPython.display import Audio
from io import BytesIO

def speak_realtime(text):
    mp3_fp = BytesIO()           # in-memory audio buffer
    tts = gTTS(text=text, lang='en', slow=False)
    tts.write_to_fp(mp3_fp)      # write audio to memory
    mp3_fp.seek(0)
    return Audio(mp3_fp.read(), autoplay=True)

# üîä SPEAK THE GENERATED VIDEO RESUME SCRIPT
speak_realtime(video_script)

# ============================================================
# FULLY GENERALISED NLP SYSTEM:
# ANY RESUME ‚Üí NEUTRAL PROFESSIONAL SUMMARY ‚Üí REAL-TIME AUDIO
# ============================================================

# ---------------- INSTALLS ----------------
!#pip install pdfplumber transformers sentencepiece gTTS torch

# ---------------- IMPORTS ----------------
import pdfplumber
import re
from google.colab import files
from transformers import pipeline
from gtts import gTTS
from IPython.display import Audio
from io import BytesIO

# ============================================================
# STEP 1: UPLOAD & EXTRACT RESUME TEXT
# ============================================================

uploaded = files.upload()
resume_path = list(uploaded.keys())[0]

def extract_resume_text(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            if page.extract_text():
                text += page.extract_text() + " "
    return text.lower()

raw_text = extract_resume_text(resume_path)

# ============================================================
# STEP 2: CLEAN TEXT (DOMAIN AGNOSTIC)
# ============================================================

def clean_text(text):
    text = re.sub(r'[‚Ä¢‚óè‚ñ™]', ' ', text)
    text = re.sub(r'[^a-z0-9., ]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

cleaned_text = clean_text(raw_text)

# ============================================================
# STEP 3: ABSTRATIVE SUMMARIZATION (GENERALIZED NLP CORE)
# ============================================================

# This model is trained on large-scale summarization data
# and works across domains (NOT engineering-specific)

summarizer = pipeline(
    "summarization",
    model="facebook/bart-large-cnn",
    device=0 if False else -1  # CPU-safe for Colab
)

# Chunk text safely (long resumes)
def chunk_text(text, max_words=450):
    words = text.split()
    return [" ".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

chunks = chunk_text(cleaned_text)

summaries = []
for chunk in chunks[:3]:  # limit for speed
    summary = summarizer(
        chunk,
        max_length=120,
        min_length=60,
        do_sample=False
    )[0]["summary_text"]
    summaries.append(summary)

final_summary = " ".join(summaries)

# ============================================================
# STEP 4: NEUTRAL SPOKEN NARRATIVE (NO ASSUMPTIONS)
# ============================================================

def build_spoken_output(summary):
    return (
        "Hello. Here is a brief professional overview.\n\n"
        f"{summary}\n\n"
        "Thank you for listening."
    )

spoken_text = build_spoken_output(final_summary)

print("\n===== GENERATED NEUTRAL PROFESSIONAL SUMMARY =====\n")
print(spoken_text)

# ============================================================
# STEP 5: REAL-TIME TEXT-TO-SPEECH (NO FILE SAVED)
# ============================================================

def speak_realtime(text):
    buffer = BytesIO()
    tts = gTTS(text=text, lang="en", slow=False)
    tts.write_to_fp(buffer)
    buffer.seek(0)
    return Audio(buffer.read(), autoplay=True)

# üîä SPEAK IMMEDIATELY
speak_realtime(spoken_text)

# ============================================================
# STREAMLIT WEB APP
# ANY RESUME ‚Üí NEUTRAL PROFESSIONAL SUMMARY ‚Üí AUDIO OUTPUT
# ============================================================


!pip install streamlit pdfplumber transformers sentencepiece torch gTTS

import streamlit as st
import pdfplumber
import re
from transformers import pipeline
from gtts import gTTS
from io import BytesIO

# ------------------------------------------------------------
# PAGE CONFIG
# ------------------------------------------------------------
st.set_page_config(
    page_title="AI Resume Narrator",
    layout="centered"
)

st.title("AI Resume Narrator")
st.write(
    "Upload **any resume PDF** to generate a neutral professional summary "
    "and hear it spoken in real time."
)

# ------------------------------------------------------------
# LOAD NLP MODEL (CACHED)
# ------------------------------------------------------------
@st.cache_resource
def load_summarizer():
    return pipeline(
        "summarization",
        model="facebook/bart-large-cnn"
    )

summarizer = load_summarizer()

# ------------------------------------------------------------
# RESUME PROCESSING FUNCTIONS
# ------------------------------------------------------------
def extract_text_from_pdf(uploaded_file):
    text = ""
    with pdfplumber.open(uploaded_file) as pdf:
        for page in pdf.pages:
            if page.extract_text():
                text += page.extract_text() + " "
    return text.lower()

def clean_text(text):
    text = re.sub(r'[‚Ä¢‚óè‚ñ™]', ' ', text)
    text = re.sub(r'[^a-z0-9., ]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def chunk_text(text, max_words=450):
    words = text.split()
    return [
        " ".join(words[i:i+max_words])
        for i in range(0, len(words), max_words)
    ]

# ------------------------------------------------------------
# TEXT ‚Üí SPOKEN AUDIO
# ------------------------------------------------------------
def text_to_audio(text):
    buffer = BytesIO()
    tts = gTTS(text=text, lang="en", slow=False)
    tts.write_to_fp(buffer)
    buffer.seek(0)
    return buffer

# ------------------------------------------------------------
# FILE UPLOAD UI
# ------------------------------------------------------------
uploaded_file = st.file_uploader(
    "Upload your resume (PDF)",
    type=["pdf"]
)

if uploaded_file:

    with st.spinner("üîç Reading resume..."):
        raw_text = extract_text_from_pdf(uploaded_file)
        cleaned_text = clean_text(raw_text)

    with st.spinner("üß† Generating professional summary..."):
        chunks = chunk_text(cleaned_text)

        summaries = []
        for chunk in chunks[:3]:  # limit for speed
            summary = summarizer(
                chunk,
                max_length=120,
                min_length=60,
                do_sample=False
            )[0]["summary_text"]
            summaries.append(summary)

        final_summary = " ".join(summaries)

    spoken_text = (
        "Hello. Here is a brief professional overview. "
        f"{final_summary} "
        "Thank you for listening."
    )

    # --------------------------------------------------------
    # DISPLAY OUTPUT
    # --------------------------------------------------------
    st.subheader("üìù Generated Professional Summary")
    st.write(spoken_text)

    # --------------------------------------------------------
    # AUDIO OUTPUT
    # --------------------------------------------------------
    if st.button("üîä Play Audio Summary"):
        audio_buffer = text_to_audio(spoken_text)
        st.audio(audio_buffer, format="audio/mp3")

# ------------------------------------------------------------
# FOOTER
# ------------------------------------------------------------
st.markdown("---")
st.caption(
    "Built using NLP-based abstractive summarization and real-time speech synthesis."
)